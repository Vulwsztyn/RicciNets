{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part III Project - Neat",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1ORnn1f9_xKUBVCwZ24Zh_E2_RERhYd4K",
      "authorship_tag": "ABX9TyMRTFnktvkkLY0YWRfKgVc2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seglass5/PartIIIProject/blob/master/Part_III_Project_Neat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQe8GUC-hjT9",
        "colab_type": "text"
      },
      "source": [
        "# 1. Install Required Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA97pKdXqnPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorboardX\n",
        "!pip install -U PyYAML\n",
        "!pip install git+https://github.com/JiaminRen/CVdevKit.git\n",
        "!pip install git+git://github.com/sovrasov/flops-counter.pytorch.git@64d38fd47cb0795437056745c64a987d944c1885\n",
        "!pip install pot\n",
        "!pip install -U networkx\n",
        "!pip install torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJwjXcYuhpwy",
        "colab_type": "text"
      },
      "source": [
        "# 2. Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q5lWaKGrXo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from ptflops import get_model_complexity_info\n",
        "import tensorboardX\n",
        "import yaml\n",
        "import CVdevKit\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import ot\n",
        "import importlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBsAJEl2hx8a",
        "colab_type": "text"
      },
      "source": [
        "# 3. Mount Drive and Import Packages and Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBsDfiJT0mEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA0L2I9lrdx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/drive/My \\Drive/Colab \\Notebooks\n",
        "\n",
        "from GraphRicciCurvature.GraphRicciCurvature.OllivierRicci2 import OllivierRicci\n",
        "\n",
        "from nodemass import undirected_to_dag\n",
        "from surgery import surgery\n",
        "from isolated_nodes import get_isolated_nodes\n",
        "from prep import prepare_new_graph_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Yb5-3uiA_u",
        "colab_type": "text"
      },
      "source": [
        "# 4. Train model of Randomly Wired Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M4p5MOfr3IK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/RandWireNN/\n",
        "!python copy_run_RandWireNN.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL1cjOBAiKlx",
        "colab_type": "text"
      },
      "source": [
        "# 5. Save graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQI0pb8Pc1HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/RandWireNN/\n",
        "# Copy and save\n",
        "conv_2_unpruned = (nx.read_graphml('./output/copy_graph/conv2.graphml',node_type=int)).copy()\n",
        "conv_3_unpruned = (nx.read_graphml('./output/copy_graph/conv3.graphml',node_type=int)).copy()\n",
        "conv_4_unpruned = (nx.read_graphml('./output/copy_graph/conv4.graphml',node_type=int)).copy()\n",
        "conv_5_unpruned = (nx.read_graphml('./output/copy_graph/conv5.graphml',node_type=int)).copy()\n",
        "\n",
        "nx.write_graphml_lxml(conv_2_unpruned, './output/copy_graph/conv2u.graphml')\n",
        "nx.write_graphml_lxml(conv_3_unpruned, './output/copy_graph/conv3u.graphml')\n",
        "nx.write_graphml_lxml(conv_4_unpruned, './output/copy_graph/conv4u.graphml')\n",
        "nx.write_graphml_lxml(conv_5_unpruned, './output/copy_graph/conv5u.graphml')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpMScFYfiScA",
        "colab_type": "text"
      },
      "source": [
        "# 6. Print and save baseline Top 1 accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdnmnfCBYGrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/RandWireNN/\n",
        "a = np.load('copy_top1.npy',allow_pickle=True)\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhmtq_FdiiEl",
        "colab_type": "text"
      },
      "source": [
        "# 7. Print and save baseline FLOPS and params "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN90thOAsGY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RUN TO LOAD #\n",
        "\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/RandWireNN/\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from utils.copy_network import Net\n",
        "from utils.graph import build_graph, save_graph, get_graph_info, load_graph\n",
        "from copy_run_RandWireNN import get_configuration\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "cfg = get_configuration()\n",
        "model = Net(cfg)\n",
        "\n",
        "#with torch.cuda.device(0):\n",
        "#  flops, params = get_model_complexity_info(model, (3, 128, 128), as_strings=False, print_per_layer_stat=False)\n",
        "  #print('{:<30} {:<8}'.format('Computational complexity: ',flops))\n",
        "  #print('{:<30} {:<8}'.format('Number of parameters: ', params))\n",
        "#  print(flops)\n",
        "#  print(params)\n",
        "\n",
        "#np.save('baseline_flops.npy',flops,allow_pickle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAU49tQejAfC",
        "colab_type": "text"
      },
      "source": [
        "# 8. Perform Ricci Flow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF8psVtPsU91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# Run to prune saved graphs #\n",
        "\n",
        "def prune_graphs(new_alpha, new_beta, new_delta):\n",
        "  %cd /content/drive/My\\ Drive/Colab\\ Notebooks/RandWireNN/output/copy_graph\n",
        "\n",
        "# Convert to directed\n",
        "\n",
        "  conv_2 = undirected_to_dag(nx.read_graphml('conv2.graphml',node_type=int))\n",
        "  conv_3 = undirected_to_dag(nx.read_graphml('conv3.graphml',node_type=int))\n",
        "  conv_4 = undirected_to_dag(nx.read_graphml('conv4.graphml',node_type=int))\n",
        "  conv_5 = undirected_to_dag(nx.read_graphml('conv5.graphml',node_type=int))\n",
        "\n",
        "# Ricci Flow on conv 2\n",
        "\n",
        "  conv_2_orc = OllivierRicci(conv_2, alpha=new_alpha, beta=new_beta, delta=new_delta, base=1, exp_power=0,proc=4, verbose=\"INFO\")\n",
        "  conv_2_orc = conv_2_orc.G.copy()\n",
        "  conv_2_orf = OllivierRicci(conv_2, alpha=new_alpha, beta=new_beta, delta=new_delta, base=1, exp_power=0,proc=4, verbose=\"INFO\")\n",
        "  conv_2_orf.compute_ricci_flow(iterations=50)\n",
        "  conv_2_rf = conv_2_orf.G.copy()\n",
        "\n",
        "# Ricci flow on conv 3 \n",
        "\n",
        "  conv_3_orc = OllivierRicci(conv_3, alpha=new_alpha, beta=new_beta, delta=new_delta, base=1, exp_power=0,proc=4, verbose=\"INFO\")\n",
        "  conv_3_orc = conv_3_orc.G.copy()\n",
        "  conv_3_orf = OllivierRicci(conv_3, alpha=new_alpha, beta=new_beta, delta=new_delta, base=1, exp_power=0,proc=4, verbose=\"INFO\")\n",
        "  conv_3_orf.compute_ricci_flow(iterations=50)\n",
        "  conv_3_rf = conv_3_orf.G.copy()\n",
        "\n",
        "# Ricci flow on conv 4\n",
        "\n",
        "  conv_4_orc = OllivierRicci(conv_4, alpha=new_alpha, beta=new_beta, delta=new_delta, base=1, exp_power=0,proc=4, verbose=\"INFO\")\n",
        "  conv_4_orc = conv_4_orc.G.copy()\n",
        "  conv_4_orf = OllivierRicci(conv_4, alpha=new_alpha, beta=new_beta, delta=new_delta, base=1, exp_power=0,proc=4, verbose=\"INFO\")\n",
        "  conv_4_orf.compute_ricci_flow(iterations=50)\n",
        "  conv_4_rf = conv_4_orf.G.copy()\n",
        "\n",
        "# Ricci flow on conv 5\n",
        "\n",
        "  conv_5_orc = OllivierRicci(conv_5, alpha=new_alpha, beta=new_beta, delta=new_delta, base=1, exp_power=0,proc=4, verbose=\"INFO\")\n",
        "  conv_5_orc = conv_5_orc.G.copy()\n",
        "  conv_5_orf = OllivierRicci(conv_5, alpha=new_alpha, beta=new_beta, delta=new_delta, base=1, exp_power=0,proc=4, verbose=\"INFO\")\n",
        "  conv_5_orf.compute_ricci_flow(iterations=50)\n",
        "  conv_5_rf = conv_5_orf.G.copy()\n",
        "  \n",
        "# Get weights find q1 and peak  \n",
        "\n",
        "  weights_2 = list(nx.get_edge_attributes(conv_2_rf,\"weight\").values())\n",
        "  #q1_2 = (np.percentile(weights_2, 25, interpolation='midpoint'))\n",
        "  q1_2 = np.mean(weights_2)# + np.std(weights_2)\n",
        "  weights_3 = list(nx.get_edge_attributes(conv_3_rf,\"weight\").values())\n",
        "  #q1_3 = (np.percentile(weights_3, 25, interpolation='midpoint'))\n",
        "  q1_3 = np.mean(weights_3)# + np.std(weights_3)\n",
        "  weights_4 = list(nx.get_edge_attributes(conv_4_rf,\"weight\").values())\n",
        "  #q1_4 = (np.percentile(weights_4, 25, interpolation='midpoint'))\n",
        "  q1_4 = np.mean(weights_4)# + np.std(weights_4)\n",
        "  weights_5 = list(nx.get_edge_attributes(conv_5_rf,\"weight\").values())\n",
        "  #q1_5 = (np.percentile(weights_5, 25, interpolation='midpoint'))\n",
        "  q1_5 = np.mean(weights_5)# + np.std(weights_5)\n",
        "\n",
        "  peak_2 = (np.max(weights_2))\n",
        "  peak_3 = (np.max(weights_3))\n",
        "  peak_4 = (np.max(weights_4))\n",
        "  peak_5 = (np.max(weights_5))\n",
        "\n",
        "# Perform surgery, getting rid of everything outside of this range\n",
        "\n",
        "  conv_2_pruned = surgery(conv_2_rf, peak_2, q1_2)\n",
        "  conv_3_pruned = surgery(conv_3_rf, peak_3, q1_3)\n",
        "  conv_4_pruned = surgery(conv_4_rf, peak_4, q1_4)\n",
        "  conv_5_pruned = surgery(conv_5_rf, peak_5, q1_5)\n",
        "\n",
        "# Prepare to feed them back into RandWire\n",
        "\n",
        "  nodes_2 = get_isolated_nodes(conv_2_pruned)\n",
        "  nodes_3 = get_isolated_nodes(conv_3_pruned)\n",
        "  nodes_4 = get_isolated_nodes(conv_4_pruned)\n",
        "  nodes_5 = get_isolated_nodes(conv_5_pruned)\n",
        "\n",
        "\n",
        "\n",
        "  new_conv_2 = prepare_new_graph_2(conv_2_pruned,nodes_2)\n",
        "  new_conv_3 = prepare_new_graph_2(conv_3_pruned,nodes_3)\n",
        "  new_conv_4 = prepare_new_graph_2(conv_4_pruned,nodes_4)\n",
        "  new_conv_5 = prepare_new_graph_2(conv_5_pruned,nodes_5)\n",
        "\n",
        "# Save the graphs\n",
        "\n",
        "  nx.write_graphml_lxml(new_conv_2, 'conv2.graphml')\n",
        "  nx.write_graphml_lxml(new_conv_3, 'conv3.graphml')\n",
        "  nx.write_graphml_lxml(new_conv_4, 'conv4.graphml')\n",
        "  nx.write_graphml_lxml(new_conv_5, 'conv5.graphml')\n",
        "\n",
        "#prune_graphs(0.5,0.5,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq-Q0WBzjaPO",
        "colab_type": "text"
      },
      "source": [
        "# 9. Policy Gradient Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSMsDtPW45Go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import sys\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2NWBmTY4n-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class policy_estimator():\n",
        "    def __init__(self, env):\n",
        "        self.n_inputs = 3\n",
        "        self.n_outputs = 1331\n",
        "        \n",
        "        # Define network\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(self.n_inputs, 128), \n",
        "            nn.ReLU(), \n",
        "            nn.Linear(128, self.n_outputs),\n",
        "            nn.Softmax(dim=-1))\n",
        "    \n",
        "    def predict(self, state):\n",
        "        action_probs = self.network(torch.FloatTensor(state))\n",
        "        return action_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLeYoRsi47EB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def action_to_params(index):\n",
        "  alpha = (index // 121)\n",
        "  beta = (index - (alpha * 121))// 11\n",
        "  delta = ((index - (alpha * 121))% 11)\n",
        "  #state = np.array([alpha/10, beta/10, delta/10])\n",
        "  alpha = alpha/10\n",
        "  beta = beta/10\n",
        "  delta = delta/10\n",
        "  return alpha, beta, delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27qaHAbj5AOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ricciParams():\n",
        "  def __init__(self):\n",
        "    # Not sure what variable I need in here, but this works\n",
        "    self.done = False\n",
        "    self.reward = 0\n",
        "    #self.alpha = 0\n",
        "    #self.beta = 0\n",
        "    #self.delta = 0\n",
        "    self.reward_threshold = 0.81\n",
        "  def reset(self):\n",
        "    %cd /content/drive/My\\ Drive/Colab\\ Notebooks/RandWireNN/output/copy_graph/\n",
        "    conv_2_unpruned = nx.read_graphml('conv2u.graphml',node_type=int)\n",
        "    conv_3_unpruned = nx.read_graphml('conv3u.graphml',node_type=int)\n",
        "    conv_4_unpruned = nx.read_graphml('conv4u.graphml',node_type=int)\n",
        "    conv_5_unpruned = nx.read_graphml('conv5u.graphml',node_type=int)\n",
        "    # Non-iterative pruning, overwrite pruned graphs with unpruned at the start of each episode\n",
        "    nx.write_graphml_lxml(conv_2_unpruned, 'conv2.graphml')\n",
        "    nx.write_graphml_lxml(conv_3_unpruned, 'conv3.graphml')\n",
        "    nx.write_graphml_lxml(conv_4_unpruned, 'conv4.graphml')\n",
        "    nx.write_graphml_lxml(conv_5_unpruned, 'conv5.graphml')\n",
        "\n",
        "    #return np.array([0,0,0])\n",
        "  # Timestep function, carry out the action specified in the argument\n",
        "  # Step is find new state according to action, prune, train, find top1 and set as reward\n",
        "  def step(self, action):\n",
        "    new_alpha, new_beta, new_delta = action_to_params(action)\n",
        "    # Prune graphs\n",
        "    prune_graphs(new_alpha, new_beta, new_delta)\n",
        "    # Train\n",
        "    %cd /content/drive/My\\ Drive/Colab\\ Notebooks/RandWireNN/\n",
        "    !python copy_run_RandWireNN.py\n",
        "    # Save Top 1 and set as reward\n",
        "    top1 = np.load('copy_top1.npy',allow_pickle = True)\n",
        "    # save reward\n",
        "    # Reward subtract mu* FLOPS/baselineFLOPS or params\n",
        "    baseline_flops = np.load('baseline_flops.npy',allow_pickle=True)\n",
        "    cfg = get_configuration()\n",
        "    model = Net(cfg)\n",
        "\n",
        "    with torch.cuda.device(0):\n",
        "      flops, params = get_model_complexity_info(model, (3, 128, 128), as_strings=False, print_per_layer_stat=False)\n",
        "\n",
        "    self.reward = ((top1/100) - (mu*(flops/baseline_flops)))\n",
        "    state = np.array([new_alpha, new_beta, new_delta])\n",
        "    \n",
        "    # Add flop penalisation, don't know how threshold changes\n",
        "    if (top1/100) < self.reward_threshold:\n",
        "      self.done = True\n",
        "\n",
        "    return state, self.reward, self.done, (top1/100) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Szd5_gy5Gbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discount_rewards(rewards, gamma=0.99):\n",
        "    r = np.array([gamma**i * rewards[i] \n",
        "        for i in range(len(rewards))])\n",
        "    # Reverse the array direction for cumsum and then\n",
        "    # revert back to the original order\n",
        "    r = r[::-1].cumsum()[::-1]\n",
        "    return r - r.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxA3YYV95LV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reinforce(env, policy_estimator, num_episodes,\n",
        "              batch_size, gamma=0.99):\n",
        "    # Set up lists to hold results\n",
        "    total_rewards = []\n",
        "    batch_rewards = []\n",
        "    batch_actions = []\n",
        "    batch_states = []\n",
        "    batch_counter = 1\n",
        "    total_top_1 = []\n",
        "    total_states = []\n",
        "    # Define optimizer\n",
        "    optimizer = optim.Adam(policy_estimator.network.parameters(), \n",
        "                           lr=0.01)\n",
        "    \n",
        "    action_space = np.arange(1331)\n",
        "    ep = 0\n",
        "    s_0 = np.array([0,0,0])\n",
        "    while ep < num_episodes:\n",
        "        env.reset()\n",
        "        states = []\n",
        "        rewards = []\n",
        "        actions = []\n",
        "        top_1s = []\n",
        "        done = False\n",
        "        while done == False:\n",
        "            # Get actions and convert to numpy array\n",
        "            action_probs = policy_estimator.predict(\n",
        "                s_0).detach().numpy()\n",
        "            print(action_probs)\n",
        "            action = np.random.choice(action_space, \n",
        "                p=action_probs)\n",
        "            print(action)\n",
        "            print(action_to_params(action))\n",
        "            s_1, r, done, top_1 = env.step(action)\n",
        "            print(s_0)\n",
        "            print(s_1)\n",
        "            states.append(s_0)\n",
        "            rewards.append(r)\n",
        "            actions.append(action)\n",
        "            top_1s.append(top_1)\n",
        "            s_0 = s_1\n",
        "            \n",
        "            # If done, batch data\n",
        "            if done:\n",
        "                batch_rewards.extend(discount_rewards(\n",
        "                    rewards, gamma))\n",
        "                batch_states.extend(states)\n",
        "                batch_actions.extend(actions)\n",
        "                batch_counter += 1\n",
        "                total_rewards.append(sum(rewards))\n",
        "                total_states.append(s_1)\n",
        "                total_top_1.append(top_1)\n",
        "                \n",
        "                # If batch is complete, update network\n",
        "                if batch_counter == batch_size:\n",
        "                    optimizer.zero_grad()\n",
        "                    state_tensor = torch.FloatTensor(batch_states)\n",
        "                    reward_tensor = torch.FloatTensor(\n",
        "                        batch_rewards)\n",
        "                    # Actions are used as indices, must be \n",
        "                    # LongTensor\n",
        "                    action_tensor = torch.LongTensor(\n",
        "                       batch_actions)\n",
        "                    \n",
        "                    # Calculate loss\n",
        "                    logprob = torch.log(\n",
        "                        policy_estimator.predict(state_tensor))\n",
        "                    selected_logprobs = reward_tensor * torch.gather(logprob, 1, action_tensor.unsqueeze(1)).squeeze()\n",
        "                    loss = -selected_logprobs.mean()\n",
        "                    \n",
        "                    # Calculate gradients\n",
        "                    loss.backward()\n",
        "                    # Apply gradients\n",
        "                    optimizer.step()\n",
        "                    \n",
        "                    batch_rewards = []\n",
        "                    batch_actions = []\n",
        "                    batch_states = []\n",
        "                    batch_counter = 1\n",
        "                    \n",
        "                avg_rewards = np.mean(total_rewards[-100:])\n",
        "                # Print running average\n",
        "                print(\"\\rEp: {} Average of last 100:\" +   \n",
        "                     \"{:.2f}\".format(\n",
        "                     ep + 1, avg_rewards), end=\"\")\n",
        "                ep += 1\n",
        "                \n",
        "    return total_rewards, total_states, total_top_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGLpaghE5aGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "mu = 0.5\n",
        "env = ricciParams()\n",
        "env.reset()\n",
        "policy_est = policy_estimator(env)\n",
        "rewards, states, top_1s = reinforce(env, policy_est, 20, 2)\n",
        "np.save('rewards030520_2.npy', rewards, allow_pickle=True)\n",
        "np.save('states030520_2.npy',states, allow_pickle=True)\n",
        "np.save('topones030520_2.npy',top_1s,allow_pickle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}